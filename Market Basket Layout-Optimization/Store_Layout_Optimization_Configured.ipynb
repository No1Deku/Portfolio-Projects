{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fb9ffd",
   "metadata": {},
   "source": [
    "# Store Layout Optimization — Configured for `Groceries data.csv`\n",
    "\n",
    "This notebook automatically detects the dataset schema and configures the Market Basket Analysis (MBA), co-purchase network, feature engineering, and uplift-simulation pipeline specifically for your `Groceries data.csv` file. Run cells sequentially. Requirements: pandas, numpy, mlxtend, xgboost, networkx, matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d64c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "pd.options.display.max_columns = 200\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = Path('/mnt/data/Groceries data.csv')\n",
    "if not file_path.exists():\n",
    "    raise FileNotFoundError('Upload Groceries data.csv to /mnt/data/')\n",
    "df = pd.read_csv(file_path)\n",
    "print('Loaded rows, cols:', df.shape)\n",
    "display(df.head(10))\n",
    "display(pd.DataFrame({'column': df.columns, 'dtype': [str(df[c].dtype) for c in df.columns]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect likely column roles (heuristic)\n",
    "cols = [c.lower() for c in df.columns]\n",
    "mappings = {}\n",
    "# transaction-like\n",
    "for candidate in ['transactionid','transaction_id','invoice','invoice_no','basketid','basket_id']:\n",
    "    if candidate in cols:\n",
    "        mappings['TransactionID'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "# date\n",
    "for candidate in ['date','transactiondate','sale_date','tdate']:\n",
    "    if candidate in cols:\n",
    "        mappings['Date'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "# item id / name / sku\n",
    "for candidate in ['itemid','item_id','productid','product_id','sku','item','product']:\n",
    "    if candidate in cols:\n",
    "        mappings['ItemID'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "for candidate in ['itemname','productname','description','item_name','product_name']:\n",
    "    if candidate in cols:\n",
    "        mappings['ItemName'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "# qty sold\n",
    "for candidate in ['qty','qty_sold','quantity','quantity_sold','units','units_sold','sales']:\n",
    "    if candidate in cols:\n",
    "        mappings['QtySold'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "# price / unit price\n",
    "for candidate in ['price','unitprice','unit_price','mrp']:\n",
    "    if candidate in cols:\n",
    "        mappings['Price'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "# store / outlet\n",
    "for candidate in ['storeid','store_id','outlet','outlet_id','store']:\n",
    "    if candidate in cols:\n",
    "        mappings['StoreID'] = df.columns[cols.index(candidate)]\n",
    "        break\n",
    "print('Detected mappings:')\n",
    "import json\n",
    "print(json.dumps(mappings, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc73981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick schema-based decisions\n",
    "is_transactional = 'TransactionID' in mappings or ('Date' in mappings and 'ItemID' in mappings and 'QtySold' in mappings)\n",
    "print('Transactional-like:', is_transactional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize column names into standard names used in this notebook\n",
    "rename_map = {}\n",
    "for k,v in mappings.items():\n",
    "    rename_map[v] = k\n",
    "df = df.rename(columns=rename_map)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d47d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning: parse dates, ensure numeric types\n",
    "if 'Date' in df.columns:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "for c in ['QtySold','Price']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66632ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare basket matrix for MBA\n",
    "if is_transactional:\n",
    "    if 'TransactionID' not in df.columns:\n",
    "        # attempt to create BasketID by grouping Date+StoreID if available\n",
    "        if 'StoreID' in df.columns and 'Date' in df.columns:\n",
    "            df['TransactionID'] = df['StoreID'].astype(str) + '|' + pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            df['TransactionID'] = pd.factorize(df.index)[0]  # fallback: unique row baskets\n",
    "    basket = df.groupby(['TransactionID','ItemID'])['QtySold'].sum().unstack(fill_value=0)\n",
    "    basket_bin = basket.applymap(lambda x: 1 if x>0 else 0)\n",
    "    print('Basket matrix created:', basket_bin.shape)\n",
    "else:\n",
    "    basket_bin = None\n",
    "    print('Dataset is not transactional or lacks required columns - MBA cannot be run automatically.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe5ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Market Basket Analysis (apriori) if possible\n",
    "if basket_bin is not None and basket_bin.shape[0]>0:\n",
    "    frequent_itemsets = apriori(basket_bin, min_support=0.01, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "    rules = rules.sort_values(['lift','support'], ascending=[False, False]).reset_index(drop=True)\n",
    "    print('Found rules:', len(rules))\n",
    "    display(rules.head(30))\n",
    "else:\n",
    "    print('Skipping MBA - insufficient basket data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build co-purchase network (top pairs)\n",
    "if 'rules' in globals() and not rules.empty:\n",
    "    pairs = rules[(rules['antecedents'].apply(len)==1) & (rules['consequents'].apply(len)==1)].copy()\n",
    "    pairs['ant'] = pairs['antecedents'].apply(lambda s: list(s)[0])\n",
    "    pairs['cons'] = pairs['consequents'].apply(lambda s: list(s)[0])\n",
    "    top_pairs = pairs.sort_values('lift', ascending=False).head(100)\n",
    "    G = nx.Graph()\n",
    "    for _,r in top_pairs.iterrows():\n",
    "        G.add_edge(r['ant'], r['cons'], weight=r['lift'])\n",
    "    plt.figure(figsize=(10,8))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    weights=[G[u][v]['weight'] for u,v in G.edges()]\n",
    "    nx.draw(G, pos, with_labels=True, node_size=300, width=[w*0.8 for w in weights])\n",
    "    plt.title('Co-purchase network (top pairs by lift)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No rules to build network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for demand modeling (daily aggregation and lags)\n",
    "agg_cols = ['ItemID']\n",
    "if 'StoreID' in df.columns:\n",
    "    agg_cols = ['StoreID','ItemID']\n",
    "\n",
    "if 'Date' in df.columns and 'QtySold' in df.columns:\n",
    "    df_daily = df.groupby(agg_cols + [pd.Grouper(key='Date')])['QtySold'].sum().reset_index()\n",
    "    df_daily = df_daily.rename(columns={'Date':'DateOnly'})\n",
    "    df_daily = df_daily.sort_values(agg_cols + ['DateOnly'])\n",
    "    for lag in [1,7,14]:\n",
    "        df_daily[f'lag_{lag}'] = df_daily.groupby(agg_cols)['QtySold'].shift(lag)\n",
    "    for window in [7,14,30]:\n",
    "        df_daily[f'rolling_mean_{window}'] = df_daily.groupby(agg_cols)['QtySold'].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "    display(df_daily.head())    \n",
    "else:\n",
    "    df_daily = None\n",
    "    print('Insufficient Date/QtySold for time-series features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04792482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model training (XGBoost) to predict QtySold — only if enough data\n",
    "if df_daily is not None:\n",
    "    df_model = df_daily.dropna(subset=['rolling_mean_7']).copy()\n",
    "    # Ensure placeholder placement/visibility features exist\n",
    "    if 'Placement' not in df.columns:\n",
    "        df['Placement'] = 'baseline'\n",
    "    # Merge placement into df_model if possible (left join)\n",
    "    if 'Placement' in df.columns:\n",
    "        df_model = df_model.merge(df[['ItemID','Placement']].drop_duplicates(), on='ItemID', how='left')\n",
    "    df_model['Placement_cat'] = df_model['Placement'].astype('category').cat.codes if 'Placement' in df_model.columns else 0\n",
    "    feature_cols = [c for c in df_model.columns if c.startswith('lag_') or c.startswith('rolling_mean_')] + ['Placement_cat']\n",
    "    df_model = df_model.dropna(subset=feature_cols + ['QtySold'])\n",
    "    split_date = df_model['DateOnly'].max() - pd.Timedelta(days=7)\n",
    "    train = df_model[df_model['DateOnly'] <= split_date]\n",
    "    test = df_model[df_model['DateOnly'] > split_date]\n",
    "    if len(train) > 50 and len(test) > 0:\n",
    "        X_train = train[feature_cols]; y_train = train['QtySold']\n",
    "        X_test = test[feature_cols]; y_test = test['QtySold']\n",
    "        model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        print('RMSE:', mean_squared_error(y_test, preds, squared=False))\n",
    "        print('R2:', r2_score(y_test, preds))\n",
    "        fi = pd.Series(model.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "        display(fi.head(20))\n",
    "    else:\n",
    "        print('Not enough rows to train model (need >50 training rows).')\n",
    "else:\n",
    "    print('No daily data for modeling.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8322343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uplift simulation helper (requires trained model and df_model)\n",
    "def simulate_placement_uplift(item_id, store_id=None, model=None, df_model=None, bump_placement=1):\n",
    "    if model is None or df_model is None:\n",
    "        raise ValueError('Provide trained model and df_model')\n",
    "    subset = df_model.copy()\n",
    "    if store_id is not None and 'StoreID' in subset.columns:\n",
    "        subset = subset[(subset['ItemID']==item_id) & (subset['StoreID']==store_id)]\n",
    "    else:\n",
    "        subset = subset[subset['ItemID']==item_id]\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    latest = subset.sort_values('DateOnly').iloc[-1:].copy()\n",
    "    X = latest[ [c for c in df_model.columns if c.startswith('lag_') or c.startswith('rolling_mean_')] + ['Placement_cat'] ].copy()\n",
    "    X_new = X.copy()\n",
    "    if 'Placement_cat' in X_new.index or 'Placement_cat' in X_new.columns:\n",
    "        # increase placement_cat code by bump (simulate moving to more visible slot)\n",
    "        X_new = X_new.copy()\n",
    "        X_new['Placement_cat'] = X_new.get('Placement_cat', 0) + bump_placement\n",
    "    pred_base = model.predict(X.values.reshape(1,-1))[0]\n",
    "    pred_new = model.predict(X_new.values.reshape(1,-1))[0]\n",
    "    uplift_pct = (pred_new - pred_base) / pred_base * 100 if pred_base != 0 else np.nan\n",
    "    return dict(item_id=item_id, store_id=store_id, pred_base=float(pred_base), pred_new=float(pred_new), uplift_pct=float(uplift_pct))\n",
    "\n",
    "print('Helper defined. Use simulate_placement_uplift(item_id, store_id, model, df_model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d156035",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Run this notebook end-to-end. It will auto-detect schema and run MBA if transactional baskets exist. If the dataset is aggregated, focus on demand modeling and placement simulation using provided features.\n",
    "- If MBA is skipped, consider transforming POS transactions into basket-level rows (TransactionID + ItemID) as input.\n",
    "\n",
    "**If you want, I can now run this notebook code on your dataset and produce the results (MBA rules, co-purchase network, model training).** Do you want me to execute the analysis now and return the outputs?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
